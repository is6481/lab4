---
title: "IS6481 | Lab 4 -- Unsupervised Learning"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# options(width = 120)
library(ggplot2)
library(fpc)
library(tidyverse)
library(reshape2)
library(hrbrthemes)

mm <- src_mysql(
  host='is6481-mysql.cffidl3kiu88.us-east-1.rds.amazonaws.com',
  port=3306,
  user='root',
  password='test1234',
  dbname='unsupervised_learning'
)
```

# About Unsupervised Learning/Cluster Analysis
Unsupervised data mining includes methods that discover unknown relationships in data. With unsupervised data mining, there’s no outcome that is being predicted; rather, patterns in the data are sought that perhaps were previously unknown. For example, one may want to find groups of customers with similar purchase patterns. In some cases, unsupervised analyses are used as the approach and the knowledge gained therefrom is presented to stakeholders.  In other cases, unsupervised analyses are used to find relationships and patterns that can, in turn, be used to build predictive models for efficiently and effectively.

Unsupervised learning algorithms are also referred to as clustering algorithms. The basic idea is to analyze the shared correlation in the data we use as an input to the algorithm. Correlation is a well known way to check if two quantitative variables are related. Cluster algorithms are more advanced as they consider the correlation between a set of variables.

What does this mean for how the algorithms relate to our desired outcome? Our desired outcome is that we are able to group the rows of a data set together in some novel way such that the rows are related via the input variables. This means that we need a set of variables that have interesting variance patterns (more on this a little later). It also means that we need to be measuring some real set of relationships AND that we have collected data that accurately represents those relationships. This means that clustering projects can fail for a few interesting reasons.

First, a cluster solution can fail (or be difficult to find) because the population of individuals we're interested in are homogeneous. I once worked on a project for a high-end auto manufacturer. It turned out that a decent solution was difficult simply because the people surveyed tended to be all in the same income bracket, cared (more or less) about the same thing, and even shared similar demographic patterns. Similar phenomena can be found if you're looking to cluster specific professions. For example, clustering data collected from doctors can be tough, especially if there is a focus on specific specialties.

Similarly, data collection can have a large effect on the ability to get a good cluster solution. I once worked on a project where we were, again, clustering data collected in a survey. The survey designer had decided to use a three point agreement scale in the questions that were to be used as inputs. This was a really bad idea since cluster algorithms live and die on the variance w/in the data. We would typically use a five point scale at a minimum and would prefer seven to eleven points as more points allow for more variance.

### About the data
This is a small dataset from 1973 on protein consumption from nine different food groups in 25 countries in Europe. The goal is to group the countries based on patterns in their protein consumption.

References:
1. Zumel, N. and Mount, J. “Practical Data Science with R”, Manning Publications, 2014.


### Explore the data
One thing often done prior to running a cluster algorithm is to explore the data. This is first done by examining summary stats as seen below.
```{r explore,message=FALSE,warning=FALSE}
d_protein <- tbl(mm,'protein') %>% collect()
summary(d_protein) %>% knitr::kable()
```

These summary stats are fine, but they don't give us a good picture into the data. It is often preferred to look at some visualizations of the data before getting started. The visualization below shows density plots for each of the variables that we plan on using as inputs into our clustering algorithm.

A density plot shows you how one column of data varies. Along the x-axis we see the full range of values that variable can take. The y-axis shows us how frequently each value occurs in the data. The density plot is, effectively, a histogram. When creating a histogram, we put the values of our variable into bins and then we count up how many data points fit in each bin. There are two primary differences between the histogram and the density plot. First, the density plot considers what happens when the bin width goes to zero. Second, the y-axis is transformed into a density function, so it no longer represents the number of data points.

As mentioned, the density plot gives you a lot of information about what each variable looks like on its own. See the density plots below for the data in question.

```{r explore_visually,fig.height=8,fig.width=10}
density_data <- d_protein %>% #select(Country,RedMeat,WhiteMeat,Eggs,Milk) %>% 
    melt(id=c('Country'),variable.name='type')
ggplot(density_data, aes(x=value)) + 
    geom_density(fill='#CC0000',alpha=0.6) + 
    facet_wrap(~type,scales='free') +
    labs(x='Value',y='Density Fuction') +
    theme_ipsum() +
    theme(panel.grid.minor.y=element_blank())
```

TODO(student) -- Answer the following questions:

* Look at the density plot for white meat, what does it mean? In other words, what does it tell you aboub white mean consumption?
* What would a bad set of density plots look like for a cluster solution?
* Do you think this set of variables will provide an interesting solution? Why?

### Consider units and scaling
The documentation for this dataset doesn’t mention what the units of measurement are, though we can assume all the columns are measured in the same units. This is important: units (or more precisely, disparity in units) affect what clusterings an algorithm will discover. If you measure vital statistics of your subjects as age in years, height in feet, and weight in pounds, you’ll get different distances—and possibly different clusters—than if you measure age in years, height in meters, and weight in kilograms.

One way to try to make the clustering more coordinate-free is to transform all the columns to have a mean value of 0 and a standard deviation of 1. This makes the standard deviation the unit of measurement in each coordinate. Assuming that your training data has a distribution that accurately represents the population at large, then a standard deviation represents approximately the same degree of difference in every coordinate. You can scale the data in R using the function scale().

we're not using Country in the kmeans clustering (not numeric)

```{r scaling_variables}
var_list <- c("RedMeat", "WhiteMeat", "Eggs", "Milk", "Fish"
                 , "Cereals", "Starch", "Nuts", "FrAndVeg") # there is a more efficient way to do this (advanced)

m_protein <- scale(d_protein[, var_list]) # filters to only the attributes we need and scales them
center_factor_list <- attr(m_protein, "scaled:center") # keep the centering factors
center_factor_list
scale_factor_list <- attr(m_protein, "scaled:scale") # keep the scaling factors
scale_factor_list
```

Build cluster model with kmeans algorithm
```{r cluster}
set.seed(42)
cluster_model <- kmeans(m_protein, centers=5)
```

Analyze the cluster model - numeric techniques
```{r analyze_clusters}
cluster_model$centers
cluster_model$size
groups <- cluster_model$cluster
groups

print_clusters <- function(labels, k){
  for(i in 1:k){
    print(paste("cluster", i))
    print(d_protein[labels==i,c("Country","RedMeat","Fish","FrAndVeg")])
  }
} 

print_clusters(groups, 5)
```

There is a pattern to the clusters: the countries in each cluster tend to be in the same geographical region. It makes sense that countries in the same region would have similar dietary habits. 

```{r centers}
cluster_model$centers
```
Note also:

* Cluster 1 contains countries with higher-than-average fish consumption (which is intuitive if the geographic region is considered)

TODO(student): Copy the following question(s) and add your response to your submission document:

Examine each cluster and make three or more similar observations

### Analyze the cluster model - visual techiques
It is a common approach to plot the first two principal components
```{r pca}
princpal_components <- prcomp(m_protein)
number_components <- 2
projection <- predict(princpal_components, newdata=m_protein)[,1:number_components]
projection_with_labels <- cbind(as.data.frame(projection),
                          cluster=as.factor(groups),
                          country=d_protein$Country)

ggplot(projection_with_labels, aes(x=PC1, y=PC2)) +
  geom_point(aes(color=cluster, size=8)) +
  geom_text(aes(label=country),
            hjust=0, vjust=1)
```

### Evaluate the cluster model
Condisider two different metrics when choosing the best value for k

* Calinski-Harabasz - the ratio of the between-cluster variance (basically the variance of all the cluster centroids from the dataset’s grand centroid) to the total within-cluster variance (basically, the average WSS of the clusters in the clustering)
* Average Silhouette Width - basically how dense the clusters are; a measure of how appropriately the data has been clustered

```{r clusterss}
cluster_model_ch <- kmeansruns(m_protein, krange=1:10, criterion="ch")
cluster_model_ch$bestk
cluster_model_asw <- kmeansruns(m_protein, krange=1:10, criterion="asw")
cluster_model_asw$bestk
```

These criteria suggest that k should be either 2 or 3.  However, see the optional text for a more advanced approach for determining the optimal # of clusters.

### Build a new cluster model with 3 clusters
TODO(student): Copy the following question(s) and add your response to your submission document:
Create a new cluster model that has only 3 clusters.  Analyze the cluster model using numeric and visual techniques similar to above.  Submit the code used to create and analyze the new cluster model. 


